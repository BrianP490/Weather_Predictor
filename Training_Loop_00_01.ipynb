{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c7208c",
   "metadata": {},
   "source": [
    "# Creating Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4ff82",
   "metadata": {},
   "source": [
    "## VERSIONS\n",
    "- 00_02: \n",
    "    - Cleanup Testing\n",
    "- 00_01: \n",
    "    - Diagnosing Exploding Gradients\n",
    "- 00_00: \n",
    "    - Initial Version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38313aff",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "10f3e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importlib.metadata import version\n",
    "import pandas as pd\n",
    "# import seaborn as sn\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Module # For type hinting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import time\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbefac4",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20070b14",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b348b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataset(Dataset):\n",
    "    \"\"\"Dataset class For the CA Weather Fire Dataset\"\"\"\n",
    "    def __init__(self, csv_file=\"./Data/CA_Weather_Fire_Dataset_Cleaned.csv\"):\n",
    "        try:\n",
    "            self.data = pd.read_csv(csv_file)   # Assign a pandas data frame\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File not found: {csv_file}\")\n",
    "\n",
    "        # Define feature and label columns\n",
    "        self.feature_columns = self.data.columns.drop(\"MAX_TEMP\")\n",
    "        self.label_column = \"MAX_TEMP\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.data.loc[index, self.feature_columns].values\n",
    "        \n",
    "        label = self.data.loc[index, self.label_column] # Extract the label for the given index\n",
    "        return (\n",
    "            torch.tensor(features, dtype=torch.float),\n",
    "            torch.tensor(label, dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2bb08",
   "metadata": {},
   "source": [
    "### Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81904364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(root_data_dir: str= \"./Data\", data_file_path: str=\"CA_Weather_Fire_Dataset_Cleaned.csv\", data_splits_dir: str=\"DataSplits\", batch_size: int=64, num_workers=0, pin_memory: bool=False, drop_last: bool=True) -> tuple[Dataset, Dataset, Dataset, DataLoader, DataLoader, DataLoader, StandardScaler]:\n",
    "    \"\"\"This function prepares the train, test, and validation datasets.\n",
    "    Args:\n",
    "        root_data_dir (str): The root of the Data Directory\n",
    "        data_file_path (str): The name of the original dataset (with .csv file extension).\n",
    "        data_splits_dir (str): Path to the train, test, and validation datasets.\n",
    "        batch_size (int): The dataloader's batch_size.\n",
    "        num_workers (int): The dataloader's number of workers.\n",
    "        pin_memory (bool): The dataloader's pin memory option.\n",
    "        drop_last (bool): The dataloader's drop_last option.\n",
    "\n",
    "    Returns: \n",
    "        train_dataset (Dataset): Dataset Class for the training dataset.\n",
    "        test_dataset (Dataset): Dataset Class for the test dataset.\n",
    "        validation_dataset (Dataset): Dataset Class for the validation dataset.\n",
    "        train_dataloader (DataLoader): The train dataloader.\n",
    "        test_dataloader (DataLoader): The test dataloader.\n",
    "        validation_dataloader (DataLoader): The validation dataloader.\n",
    "        scaler (StandardScaler): The scaler used to scale the features of the model input.\n",
    "        \"\"\"\n",
    "    \n",
    "    if not root_data_dir or not data_file_path or not data_splits_dir:  # Check for empty strings at the beginning\n",
    "        raise ValueError(\"File and directory paths cannot be empty strings.\")\n",
    "    print(f\"root_data_dir: {root_data_dir}\")\n",
    "    WEATHER_DATA_DIR = Path(root_data_dir)                  # Set the Data Root Directory\n",
    "\n",
    "    WEATHER_DATA_CLEAN_PATH = WEATHER_DATA_DIR / data_file_path # Set the path to the complete dataset\n",
    "\n",
    "    if WEATHER_DATA_CLEAN_PATH.exists():\n",
    "        print(f\"CSV file detected, reading from {WEATHER_DATA_CLEAN_PATH}\")\n",
    "        df = pd.read_csv(WEATHER_DATA_CLEAN_PATH)\n",
    "    else:\n",
    "        print(f\"Downloading csv file from HuggingFace\")\n",
    "        try:\n",
    "            df = pd.read_csv(\"hf://datasets/MaxPrestige/CA_Weather_Fire_Dataset_Cleaned/Data/CA_Weather_Fire_Dataset_Cleaned.csv\")  # Download and read the data into a pandas dataframe\n",
    "            os.makedirs(WEATHER_DATA_DIR, exist_ok=True)        # Create the Data Root Directory\n",
    "            df.to_csv(WEATHER_DATA_CLEAN_PATH, index=False)     # Save the file, omitting saving the index\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An unexpected error occurred during data download or saving: {e}\")\n",
    "    \n",
    "    DATA_SPLITS_DIR = WEATHER_DATA_DIR / data_splits_dir\n",
    "    TRAIN_DATA_PATH = DATA_SPLITS_DIR / \"train.csv\"\n",
    "    TEST_DATA_PATH = DATA_SPLITS_DIR / \"test.csv\"\n",
    "    VALIDATION_DATA_PATH = DATA_SPLITS_DIR / \"val.csv\"\n",
    "    SCALER_PATH = DATA_SPLITS_DIR / \"scaler.joblib\"\n",
    "\n",
    "    features = ['DAY_OF_YEAR', 'PRECIPITATION', 'LAGGED_PRECIPITATION', 'AVG_WIND_SPEED', 'MIN_TEMP']\n",
    "    # features = ['PRECIPITATION','AVG_WIND_SPEED', 'MIN_TEMP']\n",
    "    \n",
    "    target = 'MAX_TEMP'\n",
    "\n",
    "    if os.path.exists(TRAIN_DATA_PATH) and os.path.exists(TEST_DATA_PATH) and os.path.exists(VALIDATION_DATA_PATH) :\n",
    "        print(f\"Train, Test, and Validation csv datasets detected in '{DATA_SPLITS_DIR}', skipping generation\")\n",
    "        scaler = joblib.load(SCALER_PATH)\n",
    "    else:\n",
    "        print(f\"Datasets not found in '{DATA_SPLITS_DIR}' or incomplete. Generating datasets...\")\n",
    "        os.makedirs(DATA_SPLITS_DIR, exist_ok=True)     # Create the Data Splits Parent Directory\n",
    "        features = ['DAY_OF_YEAR', 'PRECIPITATION', 'LAGGED_PRECIPITATION', 'AVG_WIND_SPEED', 'MIN_TEMP']\n",
    "        X = df[features]\n",
    "        y = df[target]\n",
    "\n",
    "        # split your data before scaling, shuffling the data\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        X_test, X_validation, y_test, y_validation = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "        # Initialize the StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # Fit the scaler on the training data ONLY. Need to use the scaler on all inputs that the model receives.\n",
    "        # This means the mean and standard deviation are calculated from the training set.\n",
    "        scaler.fit(X_train)\n",
    "\n",
    "        # Transform the training, validation, and test data using the fitted scaler\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        X_validation_scaled = scaler.transform(X_validation)\n",
    "\n",
    "        # Save the fitted scaler object\n",
    "        try:\n",
    "            joblib.dump(scaler, SCALER_PATH)\n",
    "            print(f\"Input scaler stored in: ({SCALER_PATH})\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An unexpected error occurred when saving Scaler: {e}\")\n",
    "\n",
    "        X_train_df = pd.DataFrame(X_train_scaled, columns=features)\n",
    "        X_test_df = pd.DataFrame(X_test_scaled, columns=features)\n",
    "        X_validation_df = pd.DataFrame(X_validation_scaled, columns=features)\n",
    "\n",
    "        # Concatenate the features and labels back into a single DataFrame for each set\n",
    "        train_data_frame = pd.concat([X_train_df, y_train.reset_index(drop=True)], axis=1)\n",
    "        test_data_frame = pd.concat([X_test_df, y_test.reset_index(drop=True)], axis=1)\n",
    "        validation_data_frame = pd.concat([X_validation_df, y_validation.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        # Saving the split data to csv files\n",
    "        train_data_frame.to_csv(TRAIN_DATA_PATH, index=False)\n",
    "        test_data_frame.to_csv(TEST_DATA_PATH, index=False)\n",
    "        validation_data_frame.to_csv(VALIDATION_DATA_PATH, index=False)\n",
    "\n",
    "    print(f\"Initializing DataLoaders and Returning\")\n",
    "    # Initialize the Different Datasets\n",
    "    train_dataset = WeatherDataset(TRAIN_DATA_PATH)\n",
    "    test_dataset = WeatherDataset(TEST_DATA_PATH)\n",
    "    validation_dataset = WeatherDataset(VALIDATION_DATA_PATH)\n",
    "    # Initialize the Different DataLoaders using the Datasets\n",
    "    print(f\"Creating DataLoaders with batch_size ({batch_size}), num_workers ({num_workers}), pin_memory ({pin_memory}). Training dataset drop_last: ({drop_last})\")\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last, shuffle=True)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last)\n",
    "    validation_dataloader = DataLoader(dataset=validation_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last)\n",
    "\n",
    "    print(f\"Training DataLoader has ({len(train_dataloader)}) batches, Test DataLoader has ({len(test_dataloader)}) batches, Validation DataLoader has ({len(validation_dataloader)}) batches\")\n",
    "    \n",
    "    return (train_dataset, test_dataset, validation_dataset, train_dataloader, test_dataloader, validation_dataloader, scaler)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305fc881",
   "metadata": {},
   "source": [
    "## Agent Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982c2145",
   "metadata": {},
   "source": [
    "### Layer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c03975dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerBlock(torch.nn.Module):\n",
    "    \"\"\"Class for the individual layer blocks.\"\"\"\n",
    "    def __init__(self, intermediate_dim=32, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.Layer1 = torch.nn.Linear(in_features=intermediate_dim, out_features=intermediate_dim)\n",
    "        self.Layer_Norm1 = torch.nn.LayerNorm(normalized_shape=intermediate_dim)\n",
    "        self.ReLu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Layer1(x)\n",
    "        x = self.Layer_Norm1(x)\n",
    "        x = self.ReLu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d1f835",
   "metadata": {},
   "source": [
    "### Weather Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c24ad4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherAgent(torch.nn.Module):\n",
    "    \"\"\"Class for Agent Structure using multiple Layer Blocks.\"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.L1 = torch.nn.Linear(in_features=cfg[\"in_dim\"], out_features=cfg[\"intermediate_dim\"])\n",
    "        \n",
    "        self.Layers = torch.nn.Sequential(\n",
    "            *[LayerBlock(intermediate_dim=cfg[\"intermediate_dim\"], dropout_rate=cfg[\"dropout_rate\"]) for _ in range(cfg[\"num_blocks\"])]\n",
    "        )\n",
    "        self.out = torch.nn.Linear(in_features=cfg[\"intermediate_dim\"], out_features=cfg[\"out_dim\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.L1(x)\n",
    "        x = self.Layers(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b962ab",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdd5d1",
   "metadata": {},
   "source": [
    "### Log Iteration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c99dfaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_iteration(batch_idx: int, total_batches: int, loss_value: float):\n",
    "    \"\"\"Logs the loss of the current batch.\"\"\"\n",
    "    print(f\"Epoch batch [{batch_idx}/{total_batches}] | Loss: {loss_value:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dda76fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_epoch_iteration(epoch: int, avg_epoch_loss: float):\n",
    "    \"\"\"Log Current Metrics accumulated in the current epoch iteration.\n",
    "    Args:\n",
    "        epoch (int): the current iteration\n",
    "        avg_epoch_loss (float): The average loss of the current epoch\n",
    "    Returns:\n",
    "        N/A\n",
    "        \"\"\"\n",
    "    if avg_epoch_loss:\n",
    "        print(f\"=====================  [EPOCH ({epoch}) LOGGING]  =====================\")\n",
    "        print(\"| AVERAGES of THIS EPOCH:\")\n",
    "        print(f\"| ACCUMULATED LOSS: {avg_epoch_loss:.7f}\")\n",
    "        print(f\"===========================================================\")\n",
    "    \n",
    "    else:\n",
    "        print(\"No Data collected for this epoch to log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f752b4c1",
   "metadata": {},
   "source": [
    "### Evaluate Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41ec95e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: Module, dataloader: DataLoader, current_epoch: int = None, max_epochs: int=None, device: str = 'cpu') -> float:\n",
    "    \"\"\"\n",
    "    Evaluates the model on a given dataset and returns the average loss.\n",
    "    Args:\n",
    "        model (Module): The Model.\n",
    "        dataloader (DataLoader): The dataloader to calculate average loss with.\n",
    "        current_epoch (int): The current epoch [optional].\n",
    "        max_epochs (int): The maximum number of epochs [optional].\n",
    "        device (str): The device that the calculations will take place on.\n",
    "    Returns:\n",
    "        avg_loss (float): The calculated average loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    # loss_fn = torch.nn.MELoss(reduction='sum') # Use reduction='sum' instead of 'mean' for total loss\n",
    "    loss_fn = torch.nn.L1Loss(reduction='sum')\n",
    "    if len(dataloader.dataset) == 0:\n",
    "        print(\"Warning: Evaluation dataset is empty. Skipping evaluation.\")\n",
    "        return float('nan')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_labels in dataloader:\n",
    "            batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.unsqueeze(dim=-1).to(device)\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader.dataset)     # Calculate the average loss on the dataset\n",
    "\n",
    "    if current_epoch and max_epochs:   # If the function was called in the training loop\n",
    "        print(f\"===================  [Epoch ({current_epoch}/{max_epochs})]  ===================\")\n",
    "        print(f\"Entire Validation Dataset Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"====================================================\")\n",
    "\n",
    "    else:   # If the function was called outside of the training loop\n",
    "        print(f\"===============================================\")\n",
    "        print(f\"Entire Dataset Average Loss: {avg_loss:.4f} \")\n",
    "        print(f\"=====================================================\")\n",
    "            \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6255bf",
   "metadata": {},
   "source": [
    "### Train Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4a2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_config: dict, train_dataloader: DataLoader, validation_dataloader: DataLoader, model: WeatherAgent = None, epochs=32, learning_rate=0.0003, max_grad_norm=0.5, log_iterations=10, eval_iterations=10, device=\"cpu\") -> WeatherAgent:\n",
    "    \"\"\"The Model Training function.\n",
    "\n",
    "    Args:\n",
    "        model_config (dict): The base configurations for building the policies.\n",
    "        train_dataloader (DataLoader): The dataloader for the training loop.\n",
    "        validation_dataloader (DataLoader): The dataloader for the validation loop.\n",
    "        model (WeatherAgent): The model to be trained.\n",
    "        epochs (int): The number of times the outer loop is performed.\n",
    "        learning_rate (float): The hyperparameter that affects how much the model's parameters learn on each update iteration.\n",
    "        max_grad_norm (float): Used to promote numerical stability and prevent exploding gradients.\n",
    "        log_iterations (int): Used to log information about the state of the Agent.\n",
    "        eval_iterations (int): Used to run an evaluation of the Agent.\n",
    "        device (str): The device that the model will be trained on.\n",
    "\n",
    "    Returns: \n",
    "        agent (Module): The Trained Model in evaluation mode.\n",
    "    \"\"\"\n",
    "    print(f\"Training Model on {device} with {epochs} main epochs, {learning_rate} learning rate, max_grad_norm={max_grad_norm}.\")\n",
    "    print(f\"Logging every {log_iterations} epoch iterations, evaluating every {eval_iterations} epoch iterations.\")\n",
    "\n",
    "    agent = (model if model is not None else WeatherAgent(model_config)).to(device) # Create agent if nothing was passed, otherwise, create the agent. Send agent to device.\n",
    "\n",
    "    optimizer = torch.optim.AdamW(params=agent.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.L1Loss(reduction='mean')      # Define the Loss function\n",
    "\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "    train_dataloader_length = len(train_dataloader)\n",
    "    agent.train()   # Set agent to training mode\n",
    "    for epoch in tqdm(range(epochs), desc=f\">>>>>>>>>>>>>>>>>>>>>\\nMain Epoch (Outer Loop)\", leave=True):\n",
    "\n",
    "        epoch_loss_total = 0.0\n",
    "        for batch_idx, (inputs, labels) in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs} - Training\", leave=False)):           # Get a mini-batch of training examples from the dataloader\n",
    "            # optimizer.zero_grad(set_to_none=True)       # Clear the gradients built up; Setting to None to improve performance\n",
    "            optimizer.zero_grad()       # Clear the gradients built up; Setting to None to improve performance\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.unsqueeze(dim=-1).to(device)   # Move the inputs and labels to the device\n",
    "\n",
    "            agent_outputs = agent(inputs)       # Pass the inputs to the model and get the outputs.\n",
    "\n",
    "            loss = loss_fn(agent_outputs, labels)      # Calculate the mini-batch loss\n",
    "            epoch_loss_total += loss.item()\n",
    "            \n",
    "            loss.backward()         # Calculate the loss with respect to the model parameters\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=agent.parameters(), max_norm=max_grad_norm)   # Prevent the gradients from affecting the model parameters too much and reduce the risk of exploding gradients\n",
    "\n",
    "            optimizer.step()      # Update the model's parameters using the learning rate\n",
    "\n",
    "            # LOGGING LOSS OF CURRENT ITERATION\n",
    "            if (batch_idx + 1) % log_iterations == 0:\n",
    "                log_iteration(batch_idx=(batch_idx + 1), total_batches=train_dataloader_length, loss_value=loss.item())\n",
    "\n",
    "        # CALCULATE AND STORE THE AVERAGE EPOCH LOSS\n",
    "        epoch_avg_loss = epoch_loss_total / train_dataloader_length\n",
    "        history[\"train_loss\"].append(epoch_avg_loss)\n",
    "\n",
    "        # LOG THE AVERAGE LOSS OF THE EPOCH\n",
    "        log_epoch_iteration(epoch=epoch, avg_epoch_loss=epoch_avg_loss)\n",
    "\n",
    "        # EVALUATE THE MODEL\n",
    "        if (epoch + 1) % eval_iterations == 0:\n",
    "            val_loss = evaluate_model(model=agent, dataloader=validation_dataloader, current_epoch=(epoch + 1), max_epochs=epochs, device=device)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "            agent.train()   # Set agent to training mode\n",
    "        \n",
    "    return agent.eval(), history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4728d3db",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9396d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args) -> int:\n",
    "    print(\"SETTING UP FOR TRAINING\")\n",
    "    \n",
    "    if args.device:     # Check if the user specified to use a CPU or GPU for training\n",
    "        device = args.device\n",
    "    else:\n",
    "        if args.use_cuda:   # Check if the user wanted to use CUDA if available.\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    SAVE_LOCATION = \"./models/Weather-Agent.pt\"   # Define the model path and name of the trained model weights\n",
    "\n",
    "    BASE_CONFIG={\n",
    "    \"in_dim\": 5,\n",
    "    \"intermediate_dim\": 128,\n",
    "    \"out_dim\": 1,\n",
    "    \"num_blocks\": 12,\n",
    "    \"dropout_rate\": 0.1\n",
    "}\n",
    "\n",
    "    # --- Data Preparation Pipeline --- \n",
    "    try:\n",
    "        (train_dataset, test_dataset, validation_dataset, train_dataloader, test_dataloader, validation_dataloader, scaler) = data_pipeline(batch_size=args.dataloader_batch_size)\n",
    "    except ValueError as e:\n",
    "        print(f\"Caught an error: {e}\")\n",
    "\n",
    "    print(\"BEGINNING TRAINING SCRIPT\")\n",
    "    start_time=time.time()\n",
    "\n",
    "    trained_policy, training_history = train_model(\n",
    "        model_config=BASE_CONFIG,\n",
    "        train_dataloader=train_dataloader,\n",
    "        validation_dataloader=validation_dataloader,\n",
    "        model=None,     # Create new model\n",
    "        epochs=args.epochs,\n",
    "        learning_rate=args.learning_rate,\n",
    "        max_grad_norm=args.max_grad_norm,\n",
    "        log_iterations=args.log_iterations,\n",
    "        eval_iterations=args.eval_iterations,\n",
    "        device=device,\n",
    "    )\n",
    "    end_time=time.time()\n",
    "\n",
    "    # --- Calculate Training Time --- \n",
    "\n",
    "    elapsed_time= end_time - start_time\n",
    "    hrs = int(elapsed_time / 3600)\n",
    "    min = int((elapsed_time % 3600) / 60)\n",
    "    seconds_remaining = elapsed_time - (hrs * 3600 ) - (min * 60)\n",
    "\n",
    "    print(f\"FINISHED MODEL TRAINING. \\nTRAINING TOOK: {hrs} Hours, {min} Minutes, and {seconds_remaining:.3f} Seconds\")\n",
    "\n",
    "    # --- Testing Trained Model --- \n",
    "    print(\"\\nTESTING THE TRAINED POLICY:\")\n",
    "    test_loss = evaluate_model(model=trained_policy, dataloader=test_dataloader, current_epoch=None, max_epochs=None, device='cpu')\n",
    "\n",
    "    # ---  Saving Model Section  ---   \n",
    "\n",
    "    if args.save_model:     # Check if the user wants to save the trained model weights\n",
    "        if args.model_output_path:     # Check if the user specified a target save location\n",
    "            SAVE_LOCATION=args.model_output_path\n",
    "\n",
    "        parent_dir = os.path.dirname(SAVE_LOCATION)\n",
    "\n",
    "        # If parent_dir is empty, it means the SAVE_LOCATION is just a filename\n",
    "        # in the current directory, so no new directories need to be created.\n",
    "        if parent_dir and parent_dir != '.':\n",
    "            try:\n",
    "                os.makedirs(parent_dir, exist_ok=True)\n",
    "                print(f\"Parent directory '{parent_dir}' created to store the model.\")\n",
    "            except OSError as e:\n",
    "                print(f\"Error creating directory {parent_dir}: {e}\")\n",
    "                SAVE_LOCATION='model.pt'      # Fall back to a default save location if problem occurs.\n",
    "        \n",
    "        try:\n",
    "            torch.save(trained_policy.state_dict(), f=SAVE_LOCATION)\n",
    "            print(f\"Model weights saved in: {SAVE_LOCATION}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model to {SAVE_LOCATION}: {e}\")\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d387cce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epochs=2, learning_rate=0.003, max_grad_norm=3.0, dataloader_batch_size=64, dataloader_pin_memory=True, dataloader_num_workers=0, log_iterations=1, eval_iterations=1, use_cuda=False, device='cpu', save_model=True, model_output_path='models/Weather-Agent_01.pt')\n",
      "SETTING UP FOR TRAINING\n",
      "root_data_dir: ../Data\n",
      "CSV file detected, reading from ..\\Data\\CA_Weather_Fire_Dataset_Cleaned.csv\n",
      "Train, Test, and Validation csv datasets detected in '..\\Data\\DataSplits', skipping generation\n",
      "Initializing DataLoaders and Returning\n",
      "Creating DataLoaders with batch_size (64), num_workers (0), pin_memory (False). Training dataset drop_last: (True)\n",
      "Training DataLoader has (187) batches, Test DataLoader has (23) batches, Validation DataLoader has (23) batches\n",
      "BEGINNING TRAINING SCRIPT\n",
      "Training Model on cpu with 2 main epochs, 0.003 learning rate, max_grad_norm=3.0.\n",
      "Logging every 1 epoch iterations, evaluating every 1 epoch iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>\n",
      "Main Epoch (Outer Loop):   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [1/187] | Loss: 70.1400757\n",
      "Epoch batch [2/187] | Loss: 68.1602020\n",
      "Epoch batch [3/187] | Loss: 67.8848953\n",
      "Epoch batch [4/187] | Loss: 66.3139648\n",
      "Epoch batch [5/187] | Loss: 67.0288162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Training:   3%|▎         | 5/187 [00:00<00:10, 16.84it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [6/187] | Loss: 67.9754333\n",
      "Epoch batch [7/187] | Loss: 66.1805420\n",
      "Epoch batch [8/187] | Loss: 66.4998779\n",
      "Epoch batch [9/187] | Loss: 65.0562210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [10/187] | Loss: 66.3607101\n",
      "Epoch batch [11/187] | Loss: 66.6029587\n",
      "Epoch batch [12/187] | Loss: 66.1226349\n",
      "Epoch batch [13/187] | Loss: 67.3417892\n",
      "Epoch batch [14/187] | Loss: 65.1002121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Training:   7%|▋         | 14/187 [00:00<00:09, 18.79it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [15/187] | Loss: 63.7232895\n",
      "Epoch batch [16/187] | Loss: 63.7492867\n",
      "Epoch batch [17/187] | Loss: 64.3647919\n",
      "Epoch batch [18/187] | Loss: 63.9357376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Training:  10%|▉         | 18/187 [00:01<00:10, 16.45it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [19/187] | Loss: 65.1810226\n",
      "Epoch batch [20/187] | Loss: 63.4720764\n",
      "Epoch batch [21/187] | Loss: 62.6652184\n",
      "Epoch batch [22/187] | Loss: 62.4305687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [23/187] | Loss: 65.5961914\n",
      "Epoch batch [24/187] | Loss: 61.6426010\n",
      "Epoch batch [25/187] | Loss: 62.1015701\n",
      "Epoch batch [26/187] | Loss: 62.5686073\n",
      "Epoch batch [27/187] | Loss: 62.1772156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Training:  14%|█▍        | 27/187 [00:01<00:08, 18.55it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [28/187] | Loss: 61.0596275\n",
      "Epoch batch [29/187] | Loss: 60.3999443\n",
      "Epoch batch [30/187] | Loss: 61.7611542\n",
      "Epoch batch [31/187] | Loss: 61.2464218\n",
      "Epoch batch [32/187] | Loss: 59.8869781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [33/187] | Loss: 62.1924438\n",
      "Epoch batch [34/187] | Loss: 58.6151505\n",
      "Epoch batch [35/187] | Loss: 60.4112473\n",
      "Epoch batch [36/187] | Loss: 57.4276352\n",
      "Epoch batch [37/187] | Loss: 59.3338127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [38/187] | Loss: 58.7051086\n",
      "Epoch batch [39/187] | Loss: 59.2376938\n",
      "Epoch batch [40/187] | Loss: 57.1870003\n",
      "Epoch batch [41/187] | Loss: 58.9570274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [42/187] | Loss: 58.9952965\n",
      "Epoch batch [43/187] | Loss: 58.5663452\n",
      "Epoch batch [44/187] | Loss: 57.4245377\n",
      "Epoch batch [45/187] | Loss: 58.4157028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [46/187] | Loss: 58.0556717\n",
      "Epoch batch [47/187] | Loss: 56.2730179\n",
      "Epoch batch [48/187] | Loss: 55.6443558\n",
      "Epoch batch [49/187] | Loss: 56.6646423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [50/187] | Loss: 56.1174240\n",
      "Epoch batch [51/187] | Loss: 55.4402542\n",
      "Epoch batch [52/187] | Loss: 54.8530579\n",
      "Epoch batch [53/187] | Loss: 53.1472473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [54/187] | Loss: 54.0842285\n",
      "Epoch batch [55/187] | Loss: 54.5163116\n",
      "Epoch batch [56/187] | Loss: 51.7625809\n",
      "Epoch batch [57/187] | Loss: 54.0427017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [58/187] | Loss: 53.0251122\n",
      "Epoch batch [59/187] | Loss: 52.5202408\n",
      "Epoch batch [60/187] | Loss: 51.9548378\n",
      "Epoch batch [61/187] | Loss: 52.1106300\n",
      "Epoch batch [62/187] | Loss: 53.2005081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [63/187] | Loss: 50.2963791\n",
      "Epoch batch [64/187] | Loss: 51.5730820\n",
      "Epoch batch [65/187] | Loss: 50.3407745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Training:  35%|███▍      | 65/187 [00:03<00:08, 14.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [66/187] | Loss: 52.3080597\n",
      "Epoch batch [67/187] | Loss: 49.4994545\n",
      "Epoch batch [68/187] | Loss: 49.1236534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [69/187] | Loss: 49.0743980\n",
      "Epoch batch [70/187] | Loss: 49.9359703\n",
      "Epoch batch [71/187] | Loss: 48.4706573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [72/187] | Loss: 47.6521606\n",
      "Epoch batch [73/187] | Loss: 48.5719910\n",
      "Epoch batch [74/187] | Loss: 48.2192345\n",
      "Epoch batch [75/187] | Loss: 46.0685844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [76/187] | Loss: 47.8280640\n",
      "Epoch batch [77/187] | Loss: 45.6128998\n",
      "Epoch batch [78/187] | Loss: 45.6582794\n",
      "Epoch batch [79/187] | Loss: 45.8259010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Training:  42%|████▏     | 79/187 [00:04<00:06, 15.56it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [80/187] | Loss: 45.4121780\n",
      "Epoch batch [81/187] | Loss: 45.4136276\n",
      "Epoch batch [82/187] | Loss: 44.1586456\n",
      "Epoch batch [83/187] | Loss: 43.5690575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [84/187] | Loss: 44.4434624\n",
      "Epoch batch [85/187] | Loss: 42.0782661\n",
      "Epoch batch [86/187] | Loss: 41.8531113\n",
      "Epoch batch [87/187] | Loss: 43.8563385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [88/187] | Loss: 40.3195152\n",
      "Epoch batch [89/187] | Loss: 41.3007965\n",
      "Epoch batch [90/187] | Loss: 42.6369362\n",
      "Epoch batch [91/187] | Loss: 40.0566711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [92/187] | Loss: 41.0889893\n",
      "Epoch batch [93/187] | Loss: 37.7453995\n",
      "Epoch batch [94/187] | Loss: 40.2092514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [95/187] | Loss: 40.3295441\n",
      "Epoch batch [96/187] | Loss: 37.3151474\n",
      "Epoch batch [97/187] | Loss: 37.9390869\n",
      "Epoch batch [98/187] | Loss: 38.3960876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [99/187] | Loss: 37.4543724\n",
      "Epoch batch [100/187] | Loss: 35.7070236\n",
      "Epoch batch [101/187] | Loss: 37.5486259\n",
      "Epoch batch [102/187] | Loss: 36.2819939\n",
      "Epoch batch [103/187] | Loss: 35.5512772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [104/187] | Loss: 35.7856598\n",
      "Epoch batch [105/187] | Loss: 34.9512711\n",
      "Epoch batch [106/187] | Loss: 34.6282654\n",
      "Epoch batch [107/187] | Loss: 32.6976776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [108/187] | Loss: 32.2984924\n",
      "Epoch batch [109/187] | Loss: 32.1576958\n",
      "Epoch batch [110/187] | Loss: 31.8432999\n",
      "Epoch batch [111/187] | Loss: 31.5194702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [112/187] | Loss: 31.1202278\n",
      "Epoch batch [113/187] | Loss: 29.5696564\n",
      "Epoch batch [114/187] | Loss: 29.1379299\n",
      "Epoch batch [115/187] | Loss: 28.4384098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [116/187] | Loss: 28.7278404\n",
      "Epoch batch [117/187] | Loss: 29.0406284\n",
      "Epoch batch [118/187] | Loss: 28.6249657\n",
      "Epoch batch [119/187] | Loss: 27.1966991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [120/187] | Loss: 28.2284336\n",
      "Epoch batch [121/187] | Loss: 25.6611633\n",
      "Epoch batch [122/187] | Loss: 26.5191040\n",
      "Epoch batch [123/187] | Loss: 24.2050285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [124/187] | Loss: 26.0359802\n",
      "Epoch batch [125/187] | Loss: 23.5956192\n",
      "Epoch batch [126/187] | Loss: 24.6318169\n",
      "Epoch batch [127/187] | Loss: 25.4187145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [128/187] | Loss: 25.3627300\n",
      "Epoch batch [129/187] | Loss: 21.7063160\n",
      "Epoch batch [130/187] | Loss: 21.8021793\n",
      "Epoch batch [131/187] | Loss: 19.9983253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [132/187] | Loss: 22.3391304\n",
      "Epoch batch [133/187] | Loss: 19.6662388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [134/187] | Loss: 21.0792122\n",
      "Epoch batch [135/187] | Loss: 19.1410332\n",
      "Epoch batch [136/187] | Loss: 19.0673485\n",
      "Epoch batch [137/187] | Loss: 19.4316845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [138/187] | Loss: 19.1897545\n",
      "Epoch batch [139/187] | Loss: 18.3253841\n",
      "Epoch batch [140/187] | Loss: 18.8317795\n",
      "Epoch batch [141/187] | Loss: 17.9591160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [142/187] | Loss: 16.5005741\n",
      "Epoch batch [143/187] | Loss: 16.6606579\n",
      "Epoch batch [144/187] | Loss: 14.2185946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [145/187] | Loss: 15.4620285\n",
      "Epoch batch [146/187] | Loss: 13.8257217\n",
      "Epoch batch [147/187] | Loss: 14.6363039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [148/187] | Loss: 12.7458858\n",
      "Epoch batch [149/187] | Loss: 13.2131510\n",
      "Epoch batch [150/187] | Loss: 12.4107475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [151/187] | Loss: 10.8883438\n",
      "Epoch batch [152/187] | Loss: 12.4115906\n",
      "Epoch batch [153/187] | Loss: 9.4000340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [154/187] | Loss: 10.0288601\n",
      "Epoch batch [155/187] | Loss: 9.3276577\n",
      "Epoch batch [156/187] | Loss: 10.6778154\n",
      "Epoch batch [157/187] | Loss: 7.8205237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [158/187] | Loss: 9.9620333\n",
      "Epoch batch [159/187] | Loss: 7.7630882\n",
      "Epoch batch [160/187] | Loss: 8.3291416\n",
      "Epoch batch [161/187] | Loss: 8.5877676\n",
      "Epoch batch [162/187] | Loss: 7.0911846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Training:  87%|████████▋ | 162/187 [00:10<00:01, 16.08it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [163/187] | Loss: 7.9986954\n",
      "Epoch batch [164/187] | Loss: 6.8702445\n",
      "Epoch batch [165/187] | Loss: 7.2214222\n",
      "Epoch batch [166/187] | Loss: 7.1084528\n",
      "Epoch batch [167/187] | Loss: 6.3668976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Training:  89%|████████▉ | 167/187 [00:10<00:01, 17.67it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [168/187] | Loss: 7.4980950\n",
      "Epoch batch [169/187] | Loss: 5.5494947\n",
      "Epoch batch [170/187] | Loss: 5.2898464\n",
      "Epoch batch [171/187] | Loss: 5.8372760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [172/187] | Loss: 6.2006140\n",
      "Epoch batch [173/187] | Loss: 6.0691109\n",
      "Epoch batch [174/187] | Loss: 5.7595100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [175/187] | Loss: 6.8877854\n",
      "Epoch batch [176/187] | Loss: 5.8339510\n",
      "Epoch batch [177/187] | Loss: 6.7584357\n",
      "Epoch batch [178/187] | Loss: 6.0163589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [179/187] | Loss: 7.1420660\n",
      "Epoch batch [180/187] | Loss: 6.6053863\n",
      "Epoch batch [181/187] | Loss: 6.7224746\n",
      "Epoch batch [182/187] | Loss: 7.0827303\n",
      "Epoch batch [183/187] | Loss: 6.0128117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Training:  98%|█████████▊| 183/187 [00:11<00:00, 18.03it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [184/187] | Loss: 6.7132168\n",
      "Epoch batch [185/187] | Loss: 6.6905122\n",
      "Epoch batch [186/187] | Loss: 7.8082123\n",
      "Epoch batch [187/187] | Loss: 6.5152988\n",
      "=====================  [EPOCH (0) LOGGING]  =====================\n",
      "| AVERAGES of THIS EPOCH:\n",
      "| ACCUMULATED LOSS: 37.1272315\n",
      "===========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>\n",
      "Main Epoch (Outer Loop):  50%|█████     | 1/2 [00:12<00:12, 12.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================  [Epoch (1/2)]  ===================\n",
      "Entire Validation Dataset Average Loss: 5.3360\n",
      "====================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [1/187] | Loss: 6.9470329\n",
      "Epoch batch [2/187] | Loss: 6.5630612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [3/187] | Loss: 7.6005969\n",
      "Epoch batch [4/187] | Loss: 7.0762086\n",
      "Epoch batch [5/187] | Loss: 7.9681711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [6/187] | Loss: 7.1595964\n",
      "Epoch batch [7/187] | Loss: 6.8101010\n",
      "Epoch batch [8/187] | Loss: 6.3882542\n",
      "Epoch batch [9/187] | Loss: 5.9841256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [10/187] | Loss: 7.3917055\n",
      "Epoch batch [11/187] | Loss: 7.1466303\n",
      "Epoch batch [12/187] | Loss: 5.8305426\n",
      "Epoch batch [13/187] | Loss: 6.7870975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [14/187] | Loss: 6.8915749\n",
      "Epoch batch [15/187] | Loss: 6.5520191\n",
      "Epoch batch [16/187] | Loss: 6.3175406\n",
      "Epoch batch [17/187] | Loss: 5.6562591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [18/187] | Loss: 6.2456045\n",
      "Epoch batch [19/187] | Loss: 7.6980371\n",
      "Epoch batch [20/187] | Loss: 6.4351196\n",
      "Epoch batch [21/187] | Loss: 7.8966894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [22/187] | Loss: 6.2829542\n",
      "Epoch batch [23/187] | Loss: 6.7609091\n",
      "Epoch batch [24/187] | Loss: 5.8767281\n",
      "Epoch batch [25/187] | Loss: 6.1891518\n",
      "Epoch batch [26/187] | Loss: 5.2323112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [27/187] | Loss: 6.8411412\n",
      "Epoch batch [28/187] | Loss: 6.1003823\n",
      "Epoch batch [29/187] | Loss: 6.7653613\n",
      "Epoch batch [30/187] | Loss: 6.1123643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [31/187] | Loss: 5.4515719\n",
      "Epoch batch [32/187] | Loss: 6.0340567\n",
      "Epoch batch [33/187] | Loss: 6.3670955\n",
      "Epoch batch [34/187] | Loss: 6.9453778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [35/187] | Loss: 6.9589710\n",
      "Epoch batch [36/187] | Loss: 7.0033336\n",
      "Epoch batch [37/187] | Loss: 6.6139169\n",
      "Epoch batch [38/187] | Loss: 5.9928942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [39/187] | Loss: 6.1628380\n",
      "Epoch batch [40/187] | Loss: 5.5005307\n",
      "Epoch batch [41/187] | Loss: 5.9963827\n",
      "Epoch batch [42/187] | Loss: 6.1652722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [43/187] | Loss: 5.8862610\n",
      "Epoch batch [44/187] | Loss: 7.3021445\n",
      "Epoch batch [45/187] | Loss: 7.1701355\n",
      "Epoch batch [46/187] | Loss: 5.8235140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [47/187] | Loss: 6.0267358\n",
      "Epoch batch [48/187] | Loss: 7.3255730\n",
      "Epoch batch [49/187] | Loss: 6.6863370\n",
      "Epoch batch [50/187] | Loss: 6.5689049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [51/187] | Loss: 6.7330761\n",
      "Epoch batch [52/187] | Loss: 5.9750676\n",
      "Epoch batch [53/187] | Loss: 5.4739838\n",
      "Epoch batch [54/187] | Loss: 7.3157539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Training:  29%|██▉       | 54/187 [00:03<00:07, 17.22it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [55/187] | Loss: 5.9239264\n",
      "Epoch batch [56/187] | Loss: 5.3838186\n",
      "Epoch batch [57/187] | Loss: 6.1671677\n",
      "Epoch batch [58/187] | Loss: 6.2600117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [59/187] | Loss: 7.0201421\n",
      "Epoch batch [60/187] | Loss: 6.0317602\n",
      "Epoch batch [61/187] | Loss: 6.1082630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [62/187] | Loss: 7.1050291\n",
      "Epoch batch [63/187] | Loss: 7.2152386\n",
      "Epoch batch [64/187] | Loss: 5.9621582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [65/187] | Loss: 6.8941612\n",
      "Epoch batch [66/187] | Loss: 6.7885199\n",
      "Epoch batch [67/187] | Loss: 4.9352660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Training:  36%|███▌      | 67/187 [00:04<00:08, 13.63it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [68/187] | Loss: 6.1191835\n",
      "Epoch batch [69/187] | Loss: 5.6706910\n",
      "Epoch batch [70/187] | Loss: 6.0035696\n",
      "Epoch batch [71/187] | Loss: 7.9507551\n",
      "Epoch batch [72/187] | Loss: 6.6715741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [73/187] | Loss: 6.9274149\n",
      "Epoch batch [74/187] | Loss: 5.6983728\n",
      "Epoch batch [75/187] | Loss: 5.4890947\n",
      "Epoch batch [76/187] | Loss: 4.9205799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Training:  41%|████      | 76/187 [00:04<00:07, 15.29it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [77/187] | Loss: 6.2467327\n",
      "Epoch batch [78/187] | Loss: 6.0023975\n",
      "Epoch batch [79/187] | Loss: 6.0276175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [80/187] | Loss: 5.6457911\n",
      "Epoch batch [81/187] | Loss: 7.9319425\n",
      "Epoch batch [82/187] | Loss: 6.5496092\n",
      "Epoch batch [83/187] | Loss: 6.2406044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [84/187] | Loss: 5.7938070\n",
      "Epoch batch [85/187] | Loss: 6.7754869\n",
      "Epoch batch [86/187] | Loss: 5.1830254\n",
      "Epoch batch [87/187] | Loss: 5.9783926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [88/187] | Loss: 6.5920558\n",
      "Epoch batch [89/187] | Loss: 5.6028490\n",
      "Epoch batch [90/187] | Loss: 5.9819894\n",
      "Epoch batch [91/187] | Loss: 5.6808009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [92/187] | Loss: 5.0330572\n",
      "Epoch batch [93/187] | Loss: 5.7933750\n",
      "Epoch batch [94/187] | Loss: 6.1120548\n",
      "Epoch batch [95/187] | Loss: 5.7880025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [96/187] | Loss: 6.4506216\n",
      "Epoch batch [97/187] | Loss: 6.3593926\n",
      "Epoch batch [98/187] | Loss: 5.7347903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [99/187] | Loss: 6.4590359\n",
      "Epoch batch [100/187] | Loss: 5.7832251\n",
      "Epoch batch [101/187] | Loss: 5.3350697\n",
      "Epoch batch [102/187] | Loss: 5.1133680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [103/187] | Loss: 5.4966078\n",
      "Epoch batch [104/187] | Loss: 5.8974719\n",
      "Epoch batch [105/187] | Loss: 7.2380066\n",
      "Epoch batch [106/187] | Loss: 6.2766418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Training:  57%|█████▋    | 106/187 [00:06<00:04, 16.47it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [107/187] | Loss: 6.4270377\n",
      "Epoch batch [108/187] | Loss: 5.6208420\n",
      "Epoch batch [109/187] | Loss: 6.7607594\n",
      "Epoch batch [110/187] | Loss: 6.5223680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [111/187] | Loss: 6.0486631\n",
      "Epoch batch [112/187] | Loss: 6.7772636\n",
      "Epoch batch [113/187] | Loss: 5.9341550\n",
      "Epoch batch [114/187] | Loss: 7.1816874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [115/187] | Loss: 6.4316559\n",
      "Epoch batch [116/187] | Loss: 7.2120132\n",
      "Epoch batch [117/187] | Loss: 6.5180635\n",
      "Epoch batch [118/187] | Loss: 6.3783708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [119/187] | Loss: 4.7076645\n",
      "Epoch batch [120/187] | Loss: 6.2921238\n",
      "Epoch batch [121/187] | Loss: 5.9893823\n",
      "Epoch batch [122/187] | Loss: 5.0884104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [123/187] | Loss: 6.9014192\n",
      "Epoch batch [124/187] | Loss: 5.3937545\n",
      "Epoch batch [125/187] | Loss: 6.1784463\n",
      "Epoch batch [126/187] | Loss: 6.2069769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [127/187] | Loss: 6.4685273\n",
      "Epoch batch [128/187] | Loss: 4.7249460\n",
      "Epoch batch [129/187] | Loss: 6.2358098\n",
      "Epoch batch [130/187] | Loss: 6.4785199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [131/187] | Loss: 6.3454118\n",
      "Epoch batch [132/187] | Loss: 5.6010628\n",
      "Epoch batch [133/187] | Loss: 6.3772297\n",
      "Epoch batch [134/187] | Loss: 6.1484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [135/187] | Loss: 6.3772545\n",
      "Epoch batch [136/187] | Loss: 4.7496700\n",
      "Epoch batch [137/187] | Loss: 5.6031408\n",
      "Epoch batch [138/187] | Loss: 5.8776503\n",
      "Epoch batch [139/187] | Loss: 5.4263029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [140/187] | Loss: 5.5468678\n",
      "Epoch batch [141/187] | Loss: 6.7286777\n",
      "Epoch batch [142/187] | Loss: 6.2394381\n",
      "Epoch batch [143/187] | Loss: 5.4543867\n",
      "Epoch batch [144/187] | Loss: 4.6275854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [145/187] | Loss: 6.3616514\n",
      "Epoch batch [146/187] | Loss: 6.5450506\n",
      "Epoch batch [147/187] | Loss: 6.8846521\n",
      "Epoch batch [148/187] | Loss: 6.8152094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [149/187] | Loss: 6.9628263\n",
      "Epoch batch [150/187] | Loss: 7.2992392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [151/187] | Loss: 4.8577924\n",
      "Epoch batch [152/187] | Loss: 5.5219917\n",
      "Epoch batch [153/187] | Loss: 6.5660105\n",
      "Epoch batch [154/187] | Loss: 5.3946161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [155/187] | Loss: 5.2645493\n",
      "Epoch batch [156/187] | Loss: 6.2263260\n",
      "Epoch batch [157/187] | Loss: 7.0218711\n",
      "Epoch batch [158/187] | Loss: 5.6351895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [159/187] | Loss: 5.6761980\n",
      "Epoch batch [160/187] | Loss: 6.1377153\n",
      "Epoch batch [161/187] | Loss: 5.8515215\n",
      "Epoch batch [162/187] | Loss: 5.5965443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [163/187] | Loss: 5.9736977\n",
      "Epoch batch [164/187] | Loss: 4.8118181\n",
      "Epoch batch [165/187] | Loss: 5.7709904\n",
      "Epoch batch [166/187] | Loss: 5.3343434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [167/187] | Loss: 5.3499036\n",
      "Epoch batch [168/187] | Loss: 5.0573053\n",
      "Epoch batch [169/187] | Loss: 6.4104075\n",
      "Epoch batch [170/187] | Loss: 5.6306529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [171/187] | Loss: 5.5852776\n",
      "Epoch batch [172/187] | Loss: 5.1808095\n",
      "Epoch batch [173/187] | Loss: 5.2630191\n",
      "Epoch batch [174/187] | Loss: 5.3726945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/2 - Training:  96%|█████████▌| 179/187 [00:10<00:00, 18.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [175/187] | Loss: 6.8418489\n",
      "Epoch batch [176/187] | Loss: 5.4197249\n",
      "Epoch batch [177/187] | Loss: 6.1076665\n",
      "Epoch batch [178/187] | Loss: 6.1374264\n",
      "Epoch batch [179/187] | Loss: 5.4293580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [180/187] | Loss: 5.5705748\n",
      "Epoch batch [181/187] | Loss: 5.9578013\n",
      "Epoch batch [182/187] | Loss: 6.0178537\n",
      "Epoch batch [183/187] | Loss: 5.1916780\n",
      "Epoch batch [184/187] | Loss: 5.3249416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch batch [185/187] | Loss: 5.7195196\n",
      "Epoch batch [186/187] | Loss: 6.7445779\n",
      "Epoch batch [187/187] | Loss: 5.8553877\n",
      "=====================  [EPOCH (1) LOGGING]  =====================\n",
      "| AVERAGES of THIS EPOCH:\n",
      "| ACCUMULATED LOSS: 6.1689718\n",
      "===========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>>>>>>>>>>>>>>>p): 100%|██████████| 2/2 [00:24<00:00, 12.05s/it]\n",
      "Main Epoch (Outer Loop): 100%|██████████| 2/2 [00:24<00:00, 12.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================  [Epoch (2/2)]  ===================\n",
      "Entire Validation Dataset Average Loss: 4.6751\n",
      "====================================================\n",
      "FINISHED MODEL TRAINING. \n",
      "TRAINING TOOK: 0 Hours, 0 Minutes, and 24.189 Seconds\n",
      "\n",
      "TESTING THE TRAINED POLICY:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Entire Dataset Average Loss: 4.7193 \n",
      "=====================================================\n",
      "Parent directory 'models' created to store the model.\n",
      "Model weights saved in: models/Weather-Agent_01.pt\n",
      "FINISHED MAIN SCRIPT\n",
      "OVERALL DURATION: 0 Hours, 0 Minutes, and 24.914 Seconds\n",
      "TERMINATING PROGRAM\n"
     ]
    }
   ],
   "source": [
    "# Example usage (assuming you have a way to call this function, e.g., in a main block)\n",
    "if __name__ == '__main__':\n",
    "    # --- Begin Timing Main Script Execution Time ---\n",
    "    main_start_time=time.time()\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Train and evaluate a Regression Agent.\")\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=8,\n",
    "        help='(int, default=8) Number of training epochs to run.')\n",
    "\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.0003,\n",
    "        help='(float, default=0.0003) Learning rate used by the optimizer.')\n",
    "    \n",
    "    parser.add_argument('--max_grad_norm', type=float, default=3.0,\n",
    "        help='(float, default=3.0) The Maximum L2 Norm of the gradients for Gradient Clipping.')\n",
    "\n",
    "    parser.add_argument('--dataloader_batch_size', type=int, default=64,\n",
    "        help='(int, default=64) Batch size used by the dataloaders for training, validation, and testing.')\n",
    "\n",
    "    parser.add_argument('--dataloader_pin_memory', action='store_false',\n",
    "        help='(bool, default=True) Disable pinned memory in dataloaders (enabled by default).')\n",
    "\n",
    "    parser.add_argument('--dataloader_num_workers', type=int, default=0,\n",
    "        help='(int, default=0) Number of subprocesses to use for data loading.')\n",
    "\n",
    "    parser.add_argument('--log_iterations', type=int, default=32,\n",
    "        help='(int, default=32) Frequency (in iterations) to log training progress.')\n",
    "\n",
    "    parser.add_argument('--eval_iterations', type=int, default=32,\n",
    "        help='(int, default=32) Frequency (in iterations) to evaluate the model.')\n",
    "\n",
    "    parser.add_argument('--use_cuda', action='store_true',\n",
    "        help='(bool, default=False) Enable CUDA for training if available.')\n",
    "\n",
    "    parser.add_argument('--device', type=str, default='cpu',\n",
    "        help='(str, default=\"cpu\") Device to use for training (e.g., \"cpu\", \"cuda:0\"). Overrides --use_cuda.')\n",
    "\n",
    "    parser.add_argument('--save_model', action='store_true',\n",
    "        help='(bool, default=False) Save the trained model after training.')\n",
    "\n",
    "    parser.add_argument('--model_output_path', type=str, default='models/Weather-Agent.pt',\n",
    "        help='(str, default=\"models/Weather-Agent.pt\") File path to save the trained model.')\n",
    "\n",
    "\n",
    "    # # Parse the arguments (disabled for ipynb testing)\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    # For ipynb testing\n",
    "    simulated_args = [\n",
    "        '--epochs', '2',\n",
    "        '--learning_rate', '0.003',\n",
    "        '--log_iterations', '1',\n",
    "        '--eval_iterations', '1',\n",
    "        '--save_model',\n",
    "        '--model_output_path', 'models/Weather-Agent_01.pt'\n",
    "    ]\n",
    "    args = parser.parse_args(args=simulated_args)\n",
    "    print(args)\n",
    "    ## End of ipynb testing\n",
    "\n",
    "    ret = main(args)\n",
    "\n",
    "    main_end_time=time.time()\n",
    "\n",
    "    # --- Calculate Main Script Execution Time --- \n",
    "\n",
    "    elapsed_time= main_end_time - main_start_time\n",
    "    hrs = int(elapsed_time / 3600)\n",
    "    min = int((elapsed_time % 3600) / 60)\n",
    "    seconds_remaining = elapsed_time - (hrs * 3600 ) - (min * 60)\n",
    "\n",
    "    print(f\"FINISHED MAIN SCRIPT\\nOVERALL DURATION: {hrs} Hours, {min} Minutes, and {seconds_remaining:.3f} Seconds\")\n",
    "    if ret == 0:\n",
    "        print(\"TERMINATING PROGRAM\")\n",
    "    else: \n",
    "        print(\"Main Scipt Error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WP_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
